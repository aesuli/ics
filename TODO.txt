TODO
- duplicate classifiers
- document datasets
    * classification
- file export
    * dataset download with classification
- cleaning
    * purge unreferenced documents
    * periodically clean download directory
- batch/background processing
    * jobs update
    * classification
- check for sql/js/etc code injection
- "classifiers" page
    * accuracy estimates
- classification jobs page
- versioning
- backup of deleted data
- twitter integration
- switch to angular.js
- memory problems:
    * 64 bit fixes it?
    * queue of update/classification requests to save memory?
- richer classifier info
    * number of examples observed
    * estimated accuracy
- templates (mako)
    * footer
    * user info
    * version
- user authentication
- store source of classification (which user?)
- user authorization

TOMONITOR
- strip input strings

DONE
- lock classifiers on update
- logging
- batch/background processing
    * jobs db
    * upload
    * training with uploaded examples
- "browse and label" page
- "classifiers" page
    * simple list, info and deletion
- confirm on delete
- create classifiers from labeled examples
- file import
    * unlabeled collection upload
    * labeled collection upload
- file export
    * examples for a classifier
    * dataset download
- document datasets
    * definition in db
    * creation
    * deletion
- optimize db use
    * return feedback classifications for human-labeled docs
    * remove text duplication
    * use ids not string for classifiers and classes
- templates (mako)
    * header
- "all correct" button
- use classify, instead of score
- fix arrays [] passed to cherrypy
- db abstraction
- examples db
- serious db engine (mongodb-NO, sqlite-NO, sqlalchemy+postgres)
- fix missing cascade on labels (switched to postgres)
- richer classifier info
    * last update
